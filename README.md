# Taller 3: Spark & Arquitectura Medallion

Este taller pr√°ctico forma parte del **M√≥dulo 3: Spark** y se centra en la implementaci√≥n del **Cap√≠tulo 11: Arquitectura Medallion**.
El objetivo es simular un flujo de datos real (Lakehouse) y resolver un reto de ingenier√≠a de datos: la implementaci√≥n de una **Puerta de Calidad (Quality Gate)**.

## üéØ Objetivos del Taller

1. **Infraestructura:** Desplegar un cl√∫ster Spark completo (Master, Worker, Jupyter) usando Docker.
2. **Arquitectura:** Construir un pipeline de datos con capas **Bronce** (Crudo), **Plata** (Limpio) y **Oro** (Agregado).
3. **RETO (Quality Gate):** Modificar el flujo para detectar datos corruptos y desviarlos a una zona de **Cuarentena** en lugar de eliminarlos.

## üõ†Ô∏è Prerrequisitos

* Docker Desktop instalado y corriendo.
* Git (opcional, para clonar el repo).

---

## üöÄ Gu√≠a Paso a Paso

### 1. Despliegue del Cluster

Levanta los servicios definidos en el `docker-compose.yml`:

```bash
docker-compose up --build -d

```

Verifica que los contenedores est√©n activos:

* **Jupyter Lab (Tu entorno de trabajo):** [http://localhost:8888](https://www.google.com/search?q=http://localhost:8888)
* **Spark Master UI (Monitorizaci√≥n):** [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080)

### 2. Ingesta (Capa Bronce)

El primer paso es convertir los datos crudos a un formato optimizado (Delta Lake).

* Entra a Jupyter y abre `notebooks/01_ingest.py`.
* **Acci√≥n:** Ejecuta el script.
* **Resultado esperado:** Se crear√° la tabla Delta en `data/lakehouse/bronze/secop`.

### 3. üî• EL RETO: Transformaci√≥n y Quality Gate (Capa Plata)

Aqu√≠ aplicar√°s los conceptos del Cap√≠tulo 11. El script original `02_transform.py` simplemente borra los datos malos, lo cual es una mala pr√°ctica en auditor√≠a.

**Tu Misi√≥n:**
Modificar `02_transform.py` para implementar una l√≥gica de bifurcaci√≥n (split):

1. **Reglas de Calidad:**
* `Precio Base` debe ser mayor a 0.
* `Fecha de Firma` no puede ser nula.


2. **Ruteo de Datos:**
* ‚úÖ **Registros V√°lidos:** Guardarlos en la tabla `silver/secop`.
* ‚ùå **Registros Inv√°lidos:** Guardarlos en una nueva tabla `quarantine/secop_errors`, agregando una columna `motivo_rechazo`.



> *Pista: Utiliza funciones como `when().otherwise()` y filtros inversos (`~`) para separar los DataFrames.*

### 4. Anal√≠tica de Negocio (Capa Oro)

Con los datos saneados en la capa Plata, generaremos valor para el negocio.

* Abre y ejecuta `notebooks/03_analytics.py`.
* **Resultado:** Se generar√° una tabla agregada en `data/lakehouse/gold/top_deptos` mostrando la inversi√≥n por departamento.

---

## üìÇ Estructura del Lakehouse

El taller generar√° la siguiente estructura de carpetas en `data/lakehouse/`:

| Capa | Ruta | Descripci√≥n | Formato |
| --- | --- | --- | --- |
| **Bronce** | `/bronze/secop` | Copia fiel del CSV original (Raw). | Delta |
| **Plata** | `/silver/secop` | Datos limpios, tipados y validados. | Delta |
| **Cuarentena** | `/quarantine/secop_errors` | Datos corruptos para auditor√≠a (Resultado del Reto). | Delta |
| **Oro** | `/gold/top_deptos` | Agregaciones listas para reportes/dashboards. | Delta |

---

## üßπ Limpieza

Para detener y borrar todo (incluyendo los datos generados):

```bash
docker-compose down
# Opcional: borrar la carpeta data/lakehouse manualmente para reiniciar el taller

```
