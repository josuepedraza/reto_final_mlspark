{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5495794a-a587-4a40-81c1-9a2969f4ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe0af86-af02-44c1-b43a-8d6b0832ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/15 02:27:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Spark Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"SECOP_Regression\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.executor.memory\", \"1g\")\n",
    "         .config(\"spark.executor.cores\", \"1\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark Version:\", spark.version)\n",
    "print(\"Spark Master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e0ff03-6343-451e-9367-fccac1274d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /opt/spark-data/processed/secop_features_q4_2025.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros: 59125\n",
      "Columnas: 2\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label             |features                                                                                                                                             |\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|16.334818714984902|(75,[0,1,2,3,13,38,57,63],[0.0,0.0,0.1469782368040376,0.029582029790720565,5.951835224792248,2.561845494875754,4.595831606930893,2.0983564491217614])|\n",
      "|16.797819844969908|(75,[0,1,2,3,8,38,55,64],[0.0,0.0,0.1469782368040376,0.02702555808041138,4.870836736742472,2.561845494875754,2.17106434199968,2.4018714178938714])   |\n",
      "|17.191595272992377|(75,[0,1,2,3,5,38,56,63],[0.0,0.0,0.1469782368040376,0.1106587040319547,3.5128413176548454,2.561845494875754,2.385119396214319,2.0983564491217614])  |\n",
      "|16.077273760604726|(75,[0,1,2,3,11,38,55,63],[0.0,0.0,0.1469782368040376,0.0332341322340194,5.356433693558257,2.561845494875754,2.17106434199968,2.0983564491217614])   |\n",
      "|16.780783675579947|(75,[0,1,2,3,4,38,56,63],[0.0,0.0,0.1469782368040376,0.035790603944328585,2.1858948880308366,2.561845494875754,2.385119396214319,2.0983564491217614])|\n",
      "+------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 02:28:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "features_path = \"/opt/spark-data/processed/secop_features_q4_2025.parquet\"\n",
    "\n",
    "print(\"Leyendo:\", features_path)\n",
    "\n",
    "df = spark.read.parquet(features_path)\n",
    "\n",
    "print(\"Registros:\", df.count())\n",
    "print(\"Columnas:\", len(df.columns))\n",
    "\n",
    "df.select(\"label\", \"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90fed8b0-7dc2-4114-8c7c-6f70ddcfff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV registros: 10000\n"
     ]
    }
   ],
   "source": [
    "DEV_N = 10000  # podemos subir un poco más ahora\n",
    "df_dev = df.limit(DEV_N)\n",
    "\n",
    "print(\"DEV registros:\", df_dev.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcafb0f1-9e7a-4b10-a9b2-b105c694e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8079\n",
      "Test: 1921\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_dev.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Train:\", train_df.count())\n",
    "print(\"Test:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7fd4f5-ced9-45ee-8369-090d122b0dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo de Regresión Lineal (DEV)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 02:33:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/02/15 02:33:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "26/02/15 02:33:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo entrenado correctamente ✅\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "print(\"Entrenando modelo de Regresión Lineal (DEV)...\")\n",
    "\n",
    "# Definir modelo\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=50,\n",
    "    regParam=0.1,       # regularización (evita sobreajuste)\n",
    "    elasticNetParam=0.0 # 0 = Ridge, 1 = Lasso\n",
    ")\n",
    "\n",
    "# Entrenar modelo\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "print(\"Modelo entrenado correctamente ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61fca13a-0176-4e32-a7fc-8a3db9705855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|label             |prediction         |\n",
      "+------------------+-------------------+\n",
      "|0.0               |17.307977864928592 |\n",
      "|0.0               |19.700482506679045 |\n",
      "|0.0               |0.9124897373045435 |\n",
      "|0.0               |13.830508483426225 |\n",
      "|0.0               |-1.0660381680002047|\n",
      "|0.0               |14.15330545172078  |\n",
      "|0.0               |15.755223527521652 |\n",
      "|0.0               |17.03588902517645  |\n",
      "|0.0               |15.335752821705015 |\n",
      "|0.6931471805599453|14.182845195800107 |\n",
      "+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.7142\n",
      "MAE : 0.8532\n",
      "R2  : 0.2440\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Predecir en test\n",
    "pred = lr_model.transform(test_df)\n",
    "pred.select(\"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# Métricas\n",
    "rmse_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "mae_eval  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "r2_eval   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = rmse_eval.evaluate(pred)\n",
    "mae  = mae_eval.evaluate(pred)\n",
    "r2   = r2_eval.evaluate(pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "print(f\"R2  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5f1c48-69ce-45bc-9fb8-44f2e7084650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo R2: 0.19552595202386502\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr2 = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "lr2_model = lr2.fit(train_df)\n",
    "\n",
    "pred2 = lr2_model.transform(test_df)\n",
    "\n",
    "r2_2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(pred2)\n",
    "print(\"Nuevo R2:\", r2_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42113eaf-4f2f-46d6-acb8-749e43d520dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÉTRICAS MODELO 2 (ElasticNet) ===\n",
      "RMSE: 1.7078\n",
      "MAE : 0.7328\n",
      "R2  : 0.2446\n",
      "+------------------+------------------+\n",
      "|label             |prediction        |\n",
      "+------------------+------------------+\n",
      "|0.0               |17.41704399343791 |\n",
      "|0.0               |19.95362626809439 |\n",
      "|0.0               |0.2620865767414884|\n",
      "|0.0               |13.849034453320344|\n",
      "|0.0               |-1.717297111040729|\n",
      "|0.0               |14.191792641621435|\n",
      "|0.0               |15.855199435793345|\n",
      "|0.0               |17.114777899550194|\n",
      "|0.0               |15.456553118275894|\n",
      "|0.6931471805599453|14.24322660551498 |\n",
      "+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluadores\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Métricas\n",
    "rmse_2 = evaluator_rmse.evaluate(pred2)\n",
    "mae_2 = evaluator_mae.evaluate(pred2)\n",
    "r2_2 = evaluator_r2.evaluate(pred2)\n",
    "\n",
    "print(\"=== MÉTRICAS MODELO 2 (ElasticNet) ===\")\n",
    "print(f\"RMSE: {rmse_2:.4f}\")\n",
    "print(f\"MAE : {mae_2:.4f}\")\n",
    "print(f\"R2  : {r2_2:.4f}\")\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "pred2.select(\"label\", \"prediction\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c90360-cc3c-4b59-8eaf-00913e409246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo FINAL (FULL)...\n",
      "R2 FULL: 0.33908001080560213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/15 03:05:26 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "26/02/15 03:05:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/02/15 03:05:30 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7f5e513a[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@ac7c7f7[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@6c2a1a5d]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6ef6946e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/02/15 03:05:30 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@703f0937[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@41a142ab[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@7de55030]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6ef6946e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrenando modelo FINAL (FULL)...\")\n",
    "\n",
    "train_full, test_full = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr_final = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.5\n",
    ")\n",
    "\n",
    "lr_final_model = lr_final.fit(train_full)\n",
    "\n",
    "pred_full = lr_final_model.transform(test_full)\n",
    "\n",
    "r2_full = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(pred_full)\n",
    "\n",
    "print(\"R2 FULL:\", r2_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286ef02-f834-4d9d-8d8f-fd446a7e3ed0",
   "metadata": {},
   "source": [
    "# Análisis del Modelo de Regresión Lineal – SECOP Q4 2025\r\n",
    "\r\n",
    "En esta fase se implementó un modelo de regresión lineal para predecir el logaritmo del valor del contrato. Se utilizó una estrategia de división 80/20 para separar los datos en entrenamiento y prueba, garantizando reproducibilidad mediante una semilla fija. Siguiendo la recomendación metodológica del profesor, primero se trabajó con un subconjunto de 10.000 registros (fase DEV) para validar estabilidad y tiempos de ejecución antes de escalar al dataset completo.\r\n",
    "\r\n",
    "En la fase DEV (10.000 registros), el modelo con ElasticNet obtuvo las siguientes métricas sobre el conjunto de prueba:\r\n",
    "- **RMSE:** 1.7078  \r\n",
    "- **MAE:** 0.7328  \r\n",
    "- **R²:** 0.2446  \r\n",
    "\r\n",
    "Esto indica que el modelo logra explicar aproximadamente el **24.46% de la variabilidad** del logaritmo del valor del contrato en datos no vistos durante el entrenamiento. El error absoluto medio de 0.73 en escala logarítmica sugiere que, en promedio, la predicción se desvía menos de una unidad logarítmica respecto al valor real.\r\n",
    "\r\n",
    "Posteriormente, al entrenar el modelo con el dataset completo (59.125 registros), el desempeño mejoró, alcanzando un **R² FULL de 0.3391**, lo que implica que el modelo explica aproximadamente el **33.91% de la variabilidad** total. Este incremento es coherente con el aumento en tamaño muestral, ya que más información permite estimar con mayor precisión los parámetros del modelo.\r\n",
    "\r\n",
    "Aunque el R² no es alto, el comportamiento es consistente con la naturaleza del problema, dado que el valor de los contratos públicos depende de múltiples factores estructurales, administrativos y contextuales que no necesariamente están capturados en las variables disponibles. No se evidencia un sobreajuste severo, ya que el desempeño entre DEV y FULL mantiene coherencia y estabilidad.\r\n",
    "\r\n",
    "En conclusión, la regresión lineal regularizada constituye una línea base sólida. El pipeline es estable, reproducible y escalable, cumpliendo con los requerimientos técnicos del laboratorio. Sin embargo, el nivel de explicación alcanzado sugiere que podrían explorarse modelos no lineales en fases posteriores para capturar relaciones más complejas entre variables.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
