{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758b8fd0-6779-4241-b4f4-f3a1b3a2bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/14 19:38:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/14 19:38:55 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"test\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.range(1, 10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3bc931-0863-445d-bb14-de5cab6700d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "print(mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bd849a-779d-480b-a55d-bf72e89aa3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/spark-notebooks/01_ingesta.ipynb',\n",
       " '/opt/spark-notebooks/01_ingesta_datos.py',\n",
       " '/opt/spark-notebooks/02_exploracion_eda.py',\n",
       " '/opt/spark-notebooks/03_feature_engineering.ipynb',\n",
       " '/opt/spark-notebooks/03_feature_engineering.py',\n",
       " '/opt/spark-notebooks/04_transformaciones.py',\n",
       " '/opt/spark-notebooks/04_transformation.ipynb',\n",
       " '/opt/spark-notebooks/05_regresion_lineal.py',\n",
       " '/opt/spark-notebooks/05_regression.ipynb',\n",
       " '/opt/spark-notebooks/06_regresion_logistica.py',\n",
       " '/opt/spark-notebooks/07_regularizacion.py',\n",
       " '/opt/spark-notebooks/08_validacion_cruzada.py',\n",
       " '/opt/spark-notebooks/09_optimizacion_hiperparametros.py',\n",
       " '/opt/spark-notebooks/10_mlflow.ipynb',\n",
       " '/opt/spark-notebooks/10_mlflow_tracking.py',\n",
       " '/opt/spark-notebooks/11_model_registry.py',\n",
       " '/opt/spark-notebooks/12_inferencia_produccion.py',\n",
       " '/opt/spark-notebooks/Untitled.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob\n",
    "sorted(glob.glob(\"/opt/spark-notebooks/*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb2ca5b4-b063-445d-8585-b89e4df61efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dir exists? True\n",
      "files in data: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"data dir exists?\", os.path.exists(\"/opt/spark-data\"))\n",
    "print(\"files in data:\", os.listdir(\"/opt/spark-data\")[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a82064-bd82-4bcf-876f-7251b4bd9bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"spark master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "259b06c9-04d2-43ed-9cea-d06e089345af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking uri: http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "print(\"tracking uri:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208d4cf-96ac-49f1-816c-65581637903c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
