{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95492266-b3ef-4845-aecc-b9a67f4d221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/29 01:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/29 01:26:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, isnull\n",
    "\n",
    "# %%\n",
    "# Configurar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_FeatureEngineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d550b11c-f53b-48ef-8f9f-1fcf222fa970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros cargados: 1,000\n",
      "Columnas disponibles:\n",
      "  - referencia_del_contrato\n",
      "  - nit_entidad\n",
      "  - nombre_entidad\n",
      "  - departamento\n",
      "  - ciudad\n",
      "  - tipo_de_contrato\n",
      "  - valor_del_contrato\n",
      "  - fecha_de_firma\n",
      "  - estado_contrato\n",
      "  - valor_del_contrato_num\n",
      "  - fecha_de_firma_parsed\n",
      "  - anio\n",
      "  - mes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 01:26:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = spark.read.parquet(\"/opt/spark-data/processed/secop_eda.parquet\")\n",
    "print(f\"Registros cargados: {df.count():,}\")\n",
    "\n",
    "# %%\n",
    "# Explorar columnas disponibles\n",
    "print(\"Columnas disponibles:\")\n",
    "for col_name in df.columns:\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34206543-4446-4c66-97ef-6fff8b3440d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categóricas disponibles: ['departamento', 'tipo_de_contrato', 'estado_contrato']\n",
      "Numéricas disponibles: ['valor_del_contrato_num']\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar features para el modelo\n",
    "# Variables categóricas\n",
    "categorical_cols = [\"departamento\", \"tipo_de_contrato\", \"estado_contrato\"]\n",
    "\n",
    "# Variables numéricas\n",
    "numeric_cols = [\"plazo_de_ejec_del_contrato\", \"valor_del_contrato_num\"]\n",
    "\n",
    "# Verificar qué columnas existen\n",
    "available_cat = [c for c in categorical_cols if c in df.columns]\n",
    "available_num = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "print(f\"Categóricas disponibles: {available_cat}\")\n",
    "print(f\"Numéricas disponibles: {available_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c763b50f-d39b-4377-877b-58942c439ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros después de limpiar nulos: 1,000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Limpiar datos: eliminar nulos\n",
    "df_clean = df.dropna(subset=available_cat + available_num)\n",
    "print(f\"Registros después de limpiar nulos: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c9011c-bfe7-48f4-9e75-4b1d37274340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte strings a índices numéricos\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_idx\", handleInvalid=\"keep\")\n",
    "    for col in available_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef48791c-cb66-4163-b7e0-44eee62ea788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexers creados:\n",
      "  - departamento -> departamento_idx\n",
      "  - tipo_de_contrato -> tipo_de_contrato_idx\n",
      "  - estado_contrato -> estado_contrato_idx\n"
     ]
    }
   ],
   "source": [
    "print(\"StringIndexers creados:\")\n",
    "for idx in indexers:\n",
    "    print(f\"  - {idx.getInputCol()} -> {idx.getOutputCol()}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77abef71-a9ef-464e-84f6-904338ae8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 2: OneHotEncoder para generar variables dummy\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_idx\", outputCol=col + \"_vec\")\n",
    "    for col in available_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7467120-8d9a-41bc-8e41-0350d2c9d167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OneHotEncoders creados:\n",
      "  - departamento_idx -> departamento_vec\n",
      "  - tipo_de_contrato_idx -> tipo_de_contrato_vec\n",
      "  - estado_contrato_idx -> estado_contrato_vec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOneHotEncoders creados:\")\n",
    "for enc in encoders:\n",
    "    print(f\"  - {enc.getInputCol()} -> {enc.getOutputCol()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b83978b0-f49c-48cb-85bd-49698ed67f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VectorAssembler combinará: ['valor_del_contrato_num', 'departamento_vec', 'tipo_de_contrato_vec', 'estado_contrato_vec']\n"
     ]
    }
   ],
   "source": [
    "# PASO 3: VectorAssembler para combinar todas las features\n",
    "# Combinamos: features numéricas + features categóricas codificadas\n",
    "feature_cols = available_num + [col + \"_vec\" for col in available_cat]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVectorAssembler combinará: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bd16fb-4da5-4ba9-ad82-27c0b123122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline con 7 stages:\n",
      "  Stage 1: StringIndexer\n",
      "  Stage 2: StringIndexer\n",
      "  Stage 3: StringIndexer\n",
      "  Stage 4: OneHotEncoder\n",
      "  Stage 5: OneHotEncoder\n",
      "  Stage 6: OneHotEncoder\n",
      "  Stage 7: VectorAssembler\n"
     ]
    }
   ],
   "source": [
    "# PASO 4: Construir Pipeline\n",
    "# Pipeline = secuencia de transformaciones\n",
    "pipeline_stages = indexers + encoders + [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"\\nPipeline con {len(pipeline_stages)} stages:\")\n",
    "for i, stage in enumerate(pipeline_stages):\n",
    "    print(f\"  Stage {i+1}: {type(stage).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07289172-6083-4fbf-94c8-9495ca668bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando pipeline...\n",
      "Pipeline entrenado exitosamente\n"
     ]
    }
   ],
   "source": [
    "# PASO 5: Entrenar el pipeline (fit)\n",
    "# Nota: StringIndexer y OneHotEncoder necesitan \"aprender\" del dataset\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "print(\"Pipeline entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c3d9d4-5806-4329-9557-72df911474a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformación completada\n",
      "Columnas después de transformar: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# PASO 6: Aplicar transformaciones (transform)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformación completada\")\n",
    "print(f\"Columnas después de transformar: {len(df_transformed.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d5526dc-b473-4a2b-88fd-212209866193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features_raw: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.select(\"features_raw\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbf2fdf0-c68a-4de2-b088-7c7be3830739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        features_raw|\n",
      "+--------------------+\n",
      "|(54,[0,1,32,50],[...|\n",
      "|(54,[0,28,32,45],...|\n",
      "|(54,[0,1,32,44],[...|\n",
      "|(54,[0,11,32,44],...|\n",
      "|(54,[0,1,32,44],[...|\n",
      "|(54,[0,14,32,46],...|\n",
      "|(54,[0,1,32,48],[...|\n",
      "|(54,[0,1,32,46],[...|\n",
      "|(54,[0,1,32,46],[...|\n",
      "|(54,[0,2,32,46],[...|\n",
      "|(54,[0,2,32,45],[...|\n",
      "|(54,[0,1,32,46],[...|\n",
      "|(54,[0,6,32,46],[...|\n",
      "|(54,[0,1,32,44],[...|\n",
      "|(54,[2,32,49],[1....|\n",
      "|(54,[0,8,32,46],[...|\n",
      "|(54,[0,8,32,44],[...|\n",
      "|(54,[0,16,32,48],...|\n",
      "|(54,[0,1,32,46],[...|\n",
      "|(54,[0,1,32,46],[...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.select(\"features_raw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3fd281e-1633-441c-b003-a34c93825f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión del vector de features: 54\n",
      "+--------------------+----------------+----------------+--------------------+\n",
      "|        departamento|departamento_idx|departamento_vec|        features_raw|\n",
      "+--------------------+----------------+----------------+--------------------+\n",
      "|Distrito Capital ...|             0.0|  (31,[0],[1.0])|(54,[0,1,32,50],[...|\n",
      "|              Arauca|            27.0| (31,[27],[1.0])|(54,[0,28,32,45],...|\n",
      "|Distrito Capital ...|             0.0|  (31,[0],[1.0])|(54,[0,1,32,44],[...|\n",
      "|                Meta|            10.0| (31,[10],[1.0])|(54,[0,11,32,44],...|\n",
      "|Distrito Capital ...|             0.0|  (31,[0],[1.0])|(54,[0,1,32,44],[...|\n",
      "+--------------------+----------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver dimensión del vector de features\n",
    "sample_features = df_transformed.select(\"features_raw\").first()[0]\n",
    "print(f\"Dimensión del vector de features: {len(sample_features)}\")\n",
    "\n",
    "# %%\n",
    "# Mostrar ejemplo de transformación\n",
    "df_transformed.select(\n",
    "    available_cat[0] if available_cat else \"id\",\n",
    "    available_cat[0] + \"_idx\" if available_cat else \"id\",\n",
    "    available_cat[0] + \"_vec\" if available_cat else \"id\",\n",
    "    \"features_raw\"\n",
    ").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f585d00b-53bc-488a-898f-486bc66f4ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 01:39:37 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/metadata/_temporary]: it still exists.\n",
      "26/01/29 01:39:42 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/2_StringIndexer_33cedbd26317/metadata/_temporary]: it still exists.\n",
      "26/01/29 01:39:44 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/3_OneHotEncoder_b31de78142b9/metadata/_temporary]: it still exists.\n",
      "26/01/29 01:39:46 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/4_OneHotEncoder_352fd5ae0221/metadata/_temporary]: it still exists.\n",
      "26/01/29 01:39:48 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/5_OneHotEncoder_9bef111cff42/metadata/_temporary]: it still exists.\n",
      "26/01/29 01:39:49 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/5_OneHotEncoder_9bef111cff42/data/_temporary]: it still exists.\n",
      "26/01/29 01:39:50 WARN FileUtil: Failed to delete file or dir [/opt/spark-data/processed/feature_pipeline/stages/6_VectorAssembler_50f5ea4b7089/metadata/_temporary]: it still exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline guardado en: /opt/spark-data/processed/feature_pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset transformado guardado en: /opt/spark-data/processed/secop_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Guardar pipeline entrenado\n",
    "pipeline_path = \"/opt/spark-data/processed/feature_pipeline\"\n",
    "pipeline_model.save(pipeline_path)\n",
    "print(f\"\\nPipeline guardado en: {pipeline_path}\")\n",
    "\n",
    "# %%\n",
    "# Guardar dataset transformado\n",
    "output_path = \"/opt/spark-data/processed/secop_features.parquet\"\n",
    "df_transformed.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Dataset transformado guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff8965-9062-44ea-ba3b-bf52e86aba38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
